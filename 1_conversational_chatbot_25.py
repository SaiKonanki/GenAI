# -*- coding: utf-8 -*-
"""1. Conversational_Chatbot_25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13XnKU613qJj6J2oxhsnbMiSWrjZ-opd2
"""

from transformers import pipeline

# We'll use a conversational-finetuned model.
# 'microsoft/DialoGPT-small' is a good starting point for a simple conversational model.
# For more advanced work, you'd look at models like Llama-2-chat, Mistral-instruct, etc.,
# but they require more resources (GPU RAM) which might be an issue in free Colab.
# We'll stick to a smaller one for this initial project.
chatbot_pipeline = pipeline("text-generation", model="microsoft/DialoGPT-small")

# Initialize the conversation history. This is a list of dictionaries.
# Each dictionary represents a turn, with a "role" (user or assistant) and "content".
# This structure is common for modern conversational AI.
conversation_history = []

def chat_with_bot(user_input):
    global conversation_history # We need to modify this global list

    # 1. Add the user's input to the conversation history
    conversation_history.append({"role": "user", "content": user_input})

    # 2. Prepare the input for the model. For text-generation pipeline,
    # it often expects a single string or a list of messages.
    # We'll join the conversation for simplicity for DialoGPT,
    # but for larger models, you'd use `tokenizer.apply_chat_template`.
    # DialoGPT handles history implicitly by continuing the text.
    # For models like Llama-2, you explicitly pass the list of dicts.

    # For DialoGPT, we simply append the new input to the prior response to maintain context.
    # Let's simplify this for the pipeline by just passing the latest user input,
    # and the pipeline manages its internal state for conversation if it's a conversational model.
    # Actually, for DialoGPT through the text-generation pipeline, it works by passing the conversation history
    # as a single string. Let's adjust for that common pattern for older models.

    # Re-evaluating the best way for DialoGPT with text-generation pipeline to keep context.
    # The 'conversational' pipeline handled this directly. With 'text-generation',
    # we need to feed back the *full previous exchange* as part of the prompt.

    # Let's adjust to manually manage the string history for DialoGPT via text-generation.
    # This teaches you the underlying concept of passing context.

    current_prompt = ""
    for turn in conversation_history:
        current_prompt += f"{turn['role']}: {turn['content']}\n"
    current_prompt += "assistant:" # The model will complete this.

    # 3. Generate the response
    # We'll set max_length to be the length of the prompt + max new tokens.
    # DialoGPT doesn't explicitly use `do_sample=True` etc. in the same way
    # within the pipeline for the conversational task; it's more about continuation.
    # We'll rely on its conversational fine-tuning.

    response = chatbot_pipeline(
        current_prompt,
        max_length=len(chatbot_pipeline.tokenizer(current_prompt)['input_ids']) + 50, # Generate up to 50 new tokens
        pad_token_id=chatbot_pipeline.tokenizer.eos_token_id,
        num_return_sequences=1
    )

    # 4. Extract the generated text and identify the bot's response
    # The output will include the prompt. We need to find just the new assistant part.
    full_generated_text = response[0]['generated_text']

    # Simple way to get the new response: find where 'assistant:' ends
    # and take the text after that.
    bot_response_start_index = full_generated_text.rfind('assistant:')
    if bot_response_start_index != -1:
        bot_response = full_generated_text[bot_response_start_index + len('assistant:'):].strip()
        # Clean up any potential leftover turns if the model generated too much
        if 'user:' in bot_response:
            bot_response = bot_response.split('user:')[0].strip()
    else:
        # Fallback if 'assistant:' isn't found for some reason (shouldn't happen with this model)
        bot_response = full_generated_text.replace(current_prompt, "").strip()


    # 5. Add the bot's response to the conversation history
    conversation_history.append({"role": "assistant", "content": bot_response})

    return bot_response

# --- Let's test our basic conversational chatbot ---
print("Bot: Hello! How can I help you today?")

# First turn
user_message_1 = "Hi there, I'm looking for some career advice."
bot_reply_1 = chat_with_bot(user_message_1)
print(f"You: {user_message_1}")
print(f"Bot: {bot_reply_1}")

# Second turn - the bot should remember the context
user_message_2 = "Specifically, I'm interested in generative AI."
bot_reply_2 = chat_with_bot(user_message_2)
print(f"You: {user_message_2}")
print(f"Bot: {bot_reply_2}")

# Third turn
user_message_3 = "What kind of projects are good for a beginner?"
bot_reply_3 = chat_with_bot(user_message_3)
print(f"You: {user_message_3}")
print(f"Bot: {bot_reply_3}")

# You can keep adding more turns to test its context retention!

from huggingface_hub import notebook_login

notebook_login()

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch

# It's good practice to set TOKENIZERS_PARALLELISM to false to avoid warnings
# during pipeline instantiation, though it doesn't affect model behavior.
# import os
# os.environ["TOKENIZERS_PARALLELISM"] = "false"

# 1. Load a modern, instruction-tuned conversational model (Gemma-2B-it)
# These models are specifically trained to follow instructions and engage in chat.
# We'll also try to load it in 4-bit to save memory, which is common in professional settings
# when deploying on constrained hardware or for large models.
try:
    # Attempt 4-bit quantization for larger models if needed (though 2B might fit without it)
    # This requires 'bitsandbytes' library
    !pip install bitsandbytes accelerate -qqq

    tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b-it")
    model = AutoModelForCausalLM.from_pretrained(
        "google/gemma-2b-it",
        torch_dtype=torch.bfloat16, # Use bfloat16 for potentially faster inference and less memory
        load_in_4bit=True,          # Load in 4-bit for memory efficiency
        device_map="auto"           # Automatically assign model parts to available devices (GPU/CPU)
    )
    # Create the pipeline using the loaded model and tokenizer
    chat_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer
    )
    print("Gemma-2b-it loaded successfully with potential 4-bit quantization.")
except Exception as e:
    print(f"Could not load Gemma-2b-it with quantization: {e}. Falling back to default pipeline.")
    # Fallback if 4-bit loading fails (e.g., bitsandbytes not working on CPU only)
    chat_pipeline = pipeline("text-generation", model="google/gemma-2b-it")


# Initialize the conversation history using the proper format for instruct models
# The 'messages' format is standard in Hugging Face for instruct/chat models.
conversation_history_gemma = []

def chat_with_gemma(user_input):
    global conversation_history_gemma

    # Add the user's message to the history
    conversation_history_gemma.append({"role": "user", "content": user_input})

    # Generate response using the pipeline with the structured messages
    # Gemma's tokenizer has an 'apply_chat_template' method for this.
    # We set `return_full_text=False` to get only the new generation.
    response = chat_pipeline(
        conversation_history_gemma,
        max_new_tokens=100,  # Max new tokens for the bot's reply
        do_sample=True,      # Enable sampling for diversity
        temperature=0.7,     # Moderate creativity
        top_k=50,            # Consider top 50 most likely tokens
        top_p=0.95,          # Consider tokens whose cumulative probability is 95%
        repetition_penalty=1.2, # Penalize repetition
        return_full_text=False # Crucial for getting only the new bot response
    )

    # Extract the assistant's reply (it's the 'content' of the last message)
    bot_reply = response[0]['generated_text']

    # Add the bot's response to the conversation history
    conversation_history_gemma.append({"role": "assistant", "content": bot_reply.strip()})

    return bot_reply.strip()

# --- Let's test our enhanced conversational chatbot with Gemma ---
print("Bot (Gemma): Hello! How can I help you today?")

# First turn
user_message_1 = "Hi there, I'm looking for some career advice."
bot_reply_1 = chat_with_gemma(user_message_1)
print(f"You: {user_message_1}")
print(f"Bot: {bot_reply_1}")

# Second turn - the bot should remember the context
user_message_2 = "Specifically, I'm interested in generative AI."
bot_reply_2 = chat_with_gemma(user_message_2)
print(f"You: {user_message_2}")
print(f"Bot: {bot_reply_2}")

# Third turn
user_message_3 = "What kind of projects are good for a beginner?"
bot_reply_3 = chat_with_gemma(user_message_3)
print(f"You: {user_message_3}")
print(f"Bot: {bot_reply_3}")

# Continue the conversation
user_message_4 = "Tell me more about RAG."
bot_reply_4 = chat_with_gemma(user_message_4)
print(f"You: {user_message_4}")
print(f"Bot: {bot_reply_4}")

user_message_3 = "What is a good place to travel in Texas?"
bot_reply_3 = chat_with_gemma(user_message_3)
print(f"You: {user_message_3}")
print(f"Bot: {bot_reply_3}")










# -*- coding: utf-8 -*-
"""2. RAG_25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15bSqWBhnF-2JEhsC_WCkngXfQDhqRnpc
"""

!pip install langchain-text-splitters -qqq

from langchain_text_splitters import RecursiveCharacterTextSplitter

# Define our document (re-pasting for clarity in this code block)
rag_document_text = """
Retrieval-Augmented Generation (RAG) is an AI framework for building LLM-powered applications
that can access and interact with external data sources beyond their initial training data.
The primary goal of RAG is to enhance the factual accuracy and relevance of generated responses.

RAG systems work by first retrieving relevant information from a knowledge base
(e.g., a collection of documents, a database, or even the web) based on a user's query.
This "retrieval" step ensures the LLM has access to up-to-date and specific facts.
The retrieved information is then provided as "context" to the Large Language Model (LLM)
along with the original user query. The LLM then uses this augmented prompt to generate a grounded response.

Key benefits of RAG include reducing hallucinations, providing access to proprietary or real-time data,
and enabling source attribution. It's a cost-effective alternative to continuous fine-tuning
for updating an LLM's knowledge.

The core components of a RAG system are:
1.  **Retriever:** Responsible for finding relevant information. This usually involves:
    * Breaking documents into chunks.
    * Converting chunks into numerical vectors (embeddings) using an embedding model.
    * Storing embeddings in a vector database for efficient semantic search.
2.  **Generator:** A Large Language Model (LLM) that produces the final answer based on the retrieved context and the user's query.

Common tools used in RAG development include LangChain and LlamaIndex for orchestration,
various embedding models (e.g., OpenAI's `text-embedding-ada-002`, Google's `text-embedding-gecko`, or open-source models like `sentence-transformers`),
and vector databases (e.g., Chroma, Pinecone, FAISS).
"""

# Initialize the text splitter
# chunk_size: The maximum size of each chunk (in characters for CharacterTextSplitter, or tokens for others)
# chunk_overlap: Number of characters/tokens to overlap between chunks. Important for context.
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=150,        # A common starting point, experiment with this
    chunk_overlap=50,      # Ensures context is not lost at boundaries
    length_function=len,   # Use character length for splitting
    is_separator_regex=False, # Treat separators as plain strings
    separators=["\n\n", "\n", " ", ""] # Try splitting by paragraphs, then lines, then words, then characters
)

# Split the document into chunks
docs = text_splitter.create_documents([rag_document_text])

# Let's see the chunks
print(f"Original text length: {len(rag_document_text)} characters")
print(f"Number of chunks created: {len(docs)}")

for i, doc in enumerate(docs):
    print(f"\n--- Chunk {i+1} (Length: {len(doc.page_content)} characters) ---")
    print(doc.page_content)

!pip install langchain-community sentence-transformers chromadb -qqq

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter # Already installed, but explicit for clarity

# --- 1. Define our document (same as before) ---
rag_document_text = """
Retrieval-Augmented Generation (RAG) is an AI framework for building LLM-powered applications
that can access and interact with external data sources beyond their initial training data.
The primary goal of RAG is to enhance the factual accuracy and relevance of generated responses.

RAG systems work by first retrieving relevant information from a knowledge base
(e.g., a collection of documents, a database, or even the web) based on a user's query.
This "retrieval" step ensures the LLM has access to up-to-date and specific facts.
The retrieved information is then provided as "context" to the Large Language Model (LLM)
along with the original user query. The LLM then uses this augmented prompt to generate a grounded response.

Key benefits of RAG include reducing hallucinations, providing access to proprietary or real-time data,
and enabling source attribution. It's a cost-effective alternative to continuous fine-tuning
for updating an LLM's knowledge.

The core components of a RAG system are:
1.  **Retriever:** Responsible for finding relevant information. This usually involves:
    * Breaking documents into chunks.
    * Converting chunks into numerical vectors (embeddings) using an embedding model.
    * Storing embeddings in a vector database for efficient semantic search.
2.  **Generator:** A Large Language Model (LLM) that produces the final answer based on the retrieved context and the user's query.

Common tools used in RAG development include LangChain and LlamaIndex for orchestration,
various embedding models (e.g., OpenAI's `text-embedding-ada-002`, Google's `text-embedding-gecko`, or open-source models like `sentence-transformers`),
and vector databases (e.g., Chroma, Pinecone, FAISS).
"""

# --- 2. Text Splitting (same as our working setup) ---
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    length_function=len,
    is_separator_regex=False,
    separators=["\n\n", "\n", " ", ""]
)
docs = text_splitter.create_documents([rag_document_text])

# --- 3. Load the Embedding Model ---
# This will download the 'all-MiniLM-L6-v2' model the first time.
print("Loading embedding model: sentence-transformers/all-MiniLM-L6-v2...")
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
print("Embedding model loaded.")

# --- 4. Create a Vector Store from Chunks ---
# This step embeds all our 'docs' and stores them in Chroma in-memory.
print("Creating Chroma vector store from document chunks...")
vectorstore = Chroma.from_documents(docs, embeddings)
print("Vector store created and populated.")

# --- 5. Perform Similarity Search ---
# Now, let's ask a question and retrieve relevant chunks!
query = "What is RAG and why is it important?"
print(f"\nUser query: '{query}'")

# Perform a similarity search to get the top 3 most relevant chunks (documents)
retrieved_docs = vectorstore.similarity_search(query, k=3)

print(f"\n--- Retrieved Documents (Top {len(retrieved_docs)}) ---")
for i, doc in enumerate(retrieved_docs):
    print(f"--- Document {i+1} (Score: {doc.metadata.get('score', 'N/A')}) ---") # Note: score might not be directly available with all implementations by default
    print(doc.page_content)
    print("-" * 20) # Separator for clarity

# Let's try another query related to a potential hallucination
query_2 = "What are the core components of a RAG system?"
print(f"\nUser query: '{query_2}'")
retrieved_docs_2 = vectorstore.similarity_search(query_2, k=2)
print(f"\n--- Retrieved Documents (Top {len(retrieved_docs_2)}) ---")
for i, doc in enumerate(retrieved_docs_2):
    print(f"--- Document {i+1} (Score: {doc.metadata.get('score', 'N/A')}) ---")
    print(doc.page_content)
    print("-" * 20)

vectorstore

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
import os

# Set TOKENIZERS_PARALLELISM to false to avoid warnings
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# --- REVISED CODE FOR MODEL LOADING ---
# Before running this cell, please run the cell with notebook_login() to authenticate with Hugging Face.

# print("Loading Gemma-2b-it model without 4-bit quantization...")
# # Load tokenizer
# tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b-it")

# # Load model without load_in_4bit (using bfloat16 for efficiency)
# model = AutoModelForCausalLM.from_pretrained(
#     "google/gemma-2b-it",
#     torch_dtype=torch.bfloat16, # Still use bfloat16 for memory/speed benefits
#     device_map="auto"           # Automatically assign model parts to available devices (GPU/CPU)
# )

# # Create the pipeline using the loaded model and tokenizer
# chat_pipeline = pipeline(
#     "text-generation",
#     model=model,
#     tokenizer=tokenizer
# )
# print("Gemma-2b-it loaded successfully without 4-bit quantization.")

# --- The rest of your RAG chain code (rag_prompt_template, format_docs, rag_chain, and testing) ---
# ... (paste the rest of the RAG chain code here, starting from `from langchain_core.prompts import ChatPromptTemplate` onwards) ...

from huggingface_hub import notebook_login

notebook_login()



# Install necessary libraries (if not already installed)
!pip install langchain-community langchain-core langchain-text-splitters sentence-transformers chromadb -qqq

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
import os

# LangChain specific imports for RAG chain
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Set TOKENIZERS_PARALLELISM to false to avoid warnings
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# --- 1. Load Gemma-2b-it model (revised to avoid bitsandbytes) ---
print("Loading Gemma-2b-it model without 4-bit quantization...")
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b-it")
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2b-it",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
chat_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer
)
print("Gemma-2b-it loaded successfully without 4-bit quantization.")

# --- 2. Define our document for the knowledge base ---
rag_document_text = """
Retrieval-Augmented Generation (RAG) is an AI framework for building LLM-powered applications
that can access and interact with external data sources beyond their initial training data.
The primary goal of RAG is to enhance the factual accuracy and relevance of generated responses.

RAG systems work by first retrieving relevant information from a knowledge base
(e.g., a collection of documents, a database, or even the web) based on a user's query.
This "retrieval" step ensures the LLM has access to up-to-date and specific facts.
The retrieved information is then provided as "context" to the Large Language Model (LLM)
along with the original user query. The LLM then uses this augmented prompt to generate a grounded response.

Key benefits of RAG include reducing hallucinations, providing access to proprietary or real-time data,
and enabling source attribution. It's a cost-effective alternative to continuous fine-tuning
for updating an LLM's knowledge.

The core components of a RAG system are:
1.  **Retriever:** Responsible for finding relevant information. This usually involves:
    * Breaking documents into chunks.
    * Converting chunks into numerical vectors (embeddings) using an embedding model.
    * Storing embeddings in a vector database for efficient semantic search.
2.  **Generator:** A Large Language Model (LLM) that produces the final answer based on the retrieved context and the user's query.

Common tools used in RAG development include LangChain and LlamaIndex for orchestration,
various embedding models (e.g., OpenAI's `text-embedding-ada-002`, Google's `text-embedding-gecko`, or open-source models like `sentence-transformers`),
and vector databases (e.g., Chroma, Pinecone, FAISS).
"""

# --- 3. Text Splitting ---
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    length_function=len,
    is_separator_regex=False,
    separators=["\n\n", "\n", " ", ""]
)
docs = text_splitter.create_documents([rag_document_text])
print(f"Number of chunks created: {len(docs)}")

# --- 4. Load the Embedding Model ---
print("Loading embedding model: sentence-transformers/all-MiniLM-L6-v2...")
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
print("Embedding model loaded.")

# --- 5. Create a Vector Store from Chunks ---
print("Creating Chroma vector store from document chunks...")
vectorstore = Chroma.from_documents(docs, embeddings)
print("Vector store created and populated.")

# --- 6. Define the RAG Prompt Template (REVISED) ---
# --- REVISED RAG PROMPT TEMPLATE ---
rag_prompt_template = ChatPromptTemplate.from_messages([
    ("user",
     "You are a helpful assistant who answers questions only based on the provided context. "
     "If the answer is not found in the context, simply state that you don't know. "
     "Do not make up information.\n\n" # Added newline for clarity between instruction and context
     "Context:\n{context}\n\nQuestion: {question}")
])

# --- 7. Create the RAG Chain using LangChain Expression Language (LCEL) ---
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": vectorstore.as_retriever() | format_docs, "question": RunnablePassthrough()}
    | rag_prompt_template
    | (lambda messages: chat_pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))
    | chat_pipeline
    | StrOutputParser()
)

# --- 8. Test the RAG Chain ---
print("\n--- Testing the RAG Chatbot ---")

# Question that IS in our knowledge base
query_in_kb = "What are the core components of a RAG system?"
print(f"\nUser: {query_in_kb}")
response_in_kb = rag_chain.invoke(query_in_kb)
print(f"Bot (RAG): {response_in_kb}")
print("-" * 30)

# Question that IS NOT in our knowledge base
query_not_in_kb = "When was the first iPhone released?"
print(f"\nUser: {query_not_in_kb}")
response_not_in_kb = rag_chain.invoke(query_not_in_kb)
print(f"Bot (RAG): {response_not_in_kb}")
print("-" * 30)

# Question that was previously hallucinated by Gemma
query_rag_hallucination = "Tell me more about RAG."
print(f"\nUser: {query_rag_hallucination}")
response_rag_hallucination = rag_chain.invoke(query_rag_hallucination)
print(f"Bot (RAG): {response_rag_hallucination}")
print("-" * 30)
# -*- coding: utf-8 -*-
"""3.RAG_Gemma.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_y2YWOmH9aIl2ZdDNMsH9lKJ8JVPywtG
"""

from huggingface_hub import notebook_login
notebook_login()

# Install necessary libraries (if not already installed)
!pip install langchain-community langchain-core langchain-text-splitters sentence-transformers chromadb -qqq

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
import os

# LangChain specific imports for RAG chain
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser

# Set TOKENIZERS_PARALLELISM to false to avoid warnings
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# --- 1. Load Gemma-2b-it model (revised to avoid bitsandbytes) ---
print("Loading Gemma-2b-it model without 4-bit quantization...")
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b-it")
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2b-it",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
chat_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer
)
print("Gemma-2b-it loaded successfully without 4-bit quantization.")

# --- 2. Define our document for the knowledge base ---
rag_document_text = """
Retrieval-Augmented Generation (RAG) is an AI framework for building LLM-powered applications
that can access and interact with external data sources beyond their initial training data.
The primary goal of RAG is to enhance the factual accuracy and relevance of generated responses.

RAG systems work by first retrieving relevant information from a knowledge base
(e.g., a collection of documents, a database, or even the web) based on a user's query.
This "retrieval" step ensures the LLM has access to up-to-date and specific facts.
The retrieved information is then provided as "context" to the Large Language Model (LLM)
along with the original user query. The LLM then uses this augmented prompt to generate a grounded response.

Key benefits of RAG include reducing hallucinations, providing access to proprietary or real-time data,
and enabling source attribution. It's a cost-effective alternative to continuous fine-tuning
for updating an LLM's knowledge.

The core components of a RAG system are:
1.  **Retriever:** Responsible for finding relevant information. This usually involves:
    * Breaking documents into chunks.
    * Converting chunks into numerical vectors (embeddings) using an embedding model.
    * Storing embeddings in a vector database for efficient semantic search.
2.  **Generator:** A Large Language Model (LLM) that produces the final answer based on the retrieved context and the user's query.

Common tools used in RAG development include LangChain and LlamaIndex for orchestration,
various embedding models (e.g., OpenAI's `text-embedding-ada-002`, Google's `text-embedding-gecko`, or open-source models like `sentence-transformers`),
and vector databases (e.g., Chroma, Pinecone, FAISS).
"""

# --- 3. Text Splitting ---
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    length_function=len,
    is_separator_regex=False,
    separators=["\n\n", "\n", " ", ""]
)
docs = text_splitter.create_documents([rag_document_text])
print(f"Number of chunks created: {len(docs)}")

# --- 4. Load the Embedding Model ---
print("Loading embedding model: sentence-transformers/all-MiniLM-L6-v2...")
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
print("Embedding model loaded.")

# --- 5. Create a Vector Store from Chunks ---
print("Creating Chroma vector store from document chunks...")
vectorstore = Chroma.from_documents(docs, embeddings)
print("Vector store created and populated.")

# --- 6. Define the RAG Prompting Logic ---
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# --- 7. Create the RAG Chain using LangChain Expression Language (LCEL) (FINAL REVISED) ---
rag_chain = (
    RunnableParallel(
        context=vectorstore.as_retriever() | format_docs,
        question=RunnablePassthrough(),
    )
    | (lambda x: [
        {"role": "user", "content": (
            "You are a helpful assistant who answers questions only based on the provided context. "
            "If the answer is not found in the context, simply state that you don't know. "
            "Do not make up information.\n\n"
            f"Context:\n{x['context']}\n\nQuestion: {x['question']}"
        )}
      ])
    | (lambda messages: chat_pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))
    | chat_pipeline
    | (lambda x: x[0]['generated_text']) # <<--- NEW FINAL STEP
)

# --- 8. Test the RAG Chain ---
print("\n--- Testing the RAG Chatbot ---")

# Question that IS in our knowledge base
query_in_kb = "What are the core components of a RAG system?"
print(f"\nUser: {query_in_kb}")
response_in_kb = rag_chain.invoke(query_in_kb)
print(f"Bot (RAG): {response_in_kb}")
print("-" * 30)

# Question that IS NOT in our knowledge base
query_not_in_kb = "When was the first iPhone released?"
print(f"\nUser: {query_not_in_kb}")
response_not_in_kb = rag_chain.invoke(query_not_in_kb)
print(f"Bot (RAG): {response_not_in_kb}")
print("-" * 30)

# Question that was previously hallucinated by Gemma
query_rag_hallucination = "Tell me more about RAG."
print(f"\nUser: {query_rag_hallucination}")
response_rag_hallucination = rag_chain.invoke(query_rag_hallucination)
print(f"Bot (RAG): {response_rag_hallucination}")
print("-" * 30)